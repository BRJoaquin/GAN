{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generative Adversarial Networks (GANs) on MNIST Dataset: From Scratch to Synthesis\n",
    "\n",
    "## Introduction\n",
    "\n",
    "Welcome to this deep dive into Generative Adversarial Networks (GANs). In this notebook, we aim to implement GANs from scratch, using the MNIST dataset as our playground. While GANs have a myriad of applications, from art creation to data augmentation, our primary focus will be on understanding the architecture and mechanisms underlying these fascinating models.\n",
    "\n",
    "### What are GANs?\n",
    "\n",
    "![GANs](./assets/GANs.png)\n",
    "\n",
    "Generative Adversarial Networks (GANs) are a class of machine learning frameworks designed by Ian Goodfellow and his colleagues in 2014. They are a type of generative model, a model that learns to generate new data instances that resemble the training data. GANs are composed of two neural networks, a generator and a discriminator, that compete against each other in a zero-sum game framework. The generator tries to fool the discriminator by generating fake data instances, while the discriminator tries to distinguish between the real and fake data instances. The generator is trained to fool the discriminator, and the discriminator is trained to not be fooled by the generator. This competition between the two networks is what gives GANs their name.\n",
    "\n",
    "### Why GANs?\n",
    "\n",
    "GANs are one of the most exciting advancements in the field of machine learning and artificial intelligence in the last decade. They provide a framework for training generative models that can create data instances statistically similar to some input data. In other words, they allow us to generate new data that is similar but not identical to the data we have.\n",
    "\n",
    "### What Will We Learn?\n",
    "\n",
    "- Fundamentals of Generative Adversarial Networks\n",
    "- Custom Data Processing on the MNIST dataset\n",
    "- Creating Models from Scratch\n",
    "- Implementing Cost Functions\n",
    "- Training the GAN\n",
    "- Evaluating the Results\n",
    "\n",
    "## Dataset Used\n",
    "\n",
    "We'll use the MNIST dataset, comprising hand-written digits, to train our GAN model. This dataset is commonly used for practicing various machine learning algorithms and is readily available through PyTorch's torchvision package.\n",
    "\n",
    "As secondary data, we'll also use the FashionMNIST dataset, which is also available through torchvision. This dataset is similar to MNIST, but instead of hand-written digits, it contains images of clothing items.\n",
    "\n",
    "## Authors\n",
    "\n",
    "- [Joaquin Vigna](https://github.com/BRJoaquin)\n",
    "\n",
    "> **Note**: This notebook is also a part of an academic assignment; however, it is designed to be a comprehensive guide. Feel free to reach out if you have suggestions for improvements or find errors.\n",
    "\n",
    "## References\n",
    "\n",
    "- Original GAN Paper: [Generative Adversarial Nets](https://arxiv.org/abs/1406.2661)\n",
    "- MNIST Dataset: [MNIST Handwritten Digits](http://yann.lecun.com/exdb/mnist/)\n",
    "- FashionMNIST Dataset: [Fashion-MNIST: a Novel Image Dataset for Benchmarking Machine Learning Algorithms](https://arxiv.org/abs/1708.07747)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports\n",
    "\n",
    "In this notebook we are going to use conda to install the required packages. \n",
    "You can find the instructions to install conda [here](https://docs.conda.io/projects/conda/en/latest/user-guide/install/), and the dependencies are listed in the `environment.yml` file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PyTorch is an open-source machine learning library used for a variety of tasks,\n",
    "# but primarily for training deep neural networks.\n",
    "import torch\n",
    "\n",
    "# nn is a sub-module in PyTorch that contains useful classes and functions to build neural networks.\n",
    "import torch.nn as nn\n",
    "\n",
    "# F is a sub-module in PyTorch that contains useful functions for building neural networks.\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# DataLoader is a PyTorch utility for loading and batching data efficiently.\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "\n",
    "# torchvision contains various utilities, pre-trained models, and datasets specifically\n",
    "# geared towards computer vision tasks.\n",
    "import torchvision\n",
    "\n",
    "# datasets are a set of common datasets used for computer vision tasks.\n",
    "# transforms are a set of common image transformations that are often required when\n",
    "# working with image data.\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "# ImageFolder is a utility for loading images directly from a directory structure where\n",
    "# each sub-directory represents a different class.\n",
    "from torchvision.datasets import ImageFolder\n",
    "\n",
    "# random_split is a utility function to randomly split a dataset into non-overlapping\n",
    "# new datasets of given lengths.\n",
    "from torch.utils.data import random_split\n",
    "\n",
    "# SummaryWriter is a PyTorch utility for logging information to be displayed in TensorBoard.\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "# summary is a PyTorch utility for displaying the summary of a PyTorch model.\n",
    "from torchinfo import summary\n",
    "\n",
    "# tqdm is a Python library that adds a progress bar to an iterable object.\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Matplotlib is a plotting library that is useful for visualizing data, plotting graphs, etc.\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# NumPy is a library for numerical operations and is especially useful for array and\n",
    "# matrix computations.\n",
    "import numpy as np\n",
    "\n",
    "# PIL is a library for image processing.\n",
    "from PIL import Image\n",
    "\n",
    "# os is a Python module that provides a portable way of using operating system dependent\n",
    "import os\n",
    "\n",
    "# time is a module that provides various time-related functions.\n",
    "import time\n",
    "\n",
    "# random is a module that implements pseudo-random number generators for various distributions.\n",
    "import random\n",
    "\n",
    "# accuracy_score computes the accuracy classification score.\n",
    "# confusion_matrix computes confusion matrix to evaluate the accuracy of a classification.\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    confusion_matrix,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    f1_score,\n",
    ")\n",
    "\n",
    "# itertools is a module that provides various functions that work on iterators to produce\n",
    "from itertools import product\n",
    "\n",
    "# math is a module that provides access to the mathematical functions.\n",
    "import math\n",
    "\n",
    "# Counter is a dictionary subclass for counting hashable objects.\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checking CUDA Availability\n",
    "\n",
    "In deep learning projects, it's common to leverage the power of GPUs for computation. CUDA is a parallel computing platform that allows us to use the GPU for these intensive calculations. The following code snippet checks if CUDA is available on the machine. If CUDA is available, it sets the device to \"cuda\"; otherwise, it falls back to using the CPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if CUDA is available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Print the device being used\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting the Seed\n",
    "\n",
    "Setting the seed ensures that the results are reproducible. This is important for debugging and testing purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Exploration\n",
    "\n",
    "Before diving into the complexities of Generative Adversarial Networks, it's essential to have a solid understanding of the dataset we'll be working with. The MNIST dataset contains 28x28 grayscale images of handwritten digits (0 through 9).\n",
    "\n",
    "### Objectives\n",
    "\n",
    "1. Load the MNIST dataset from PyTorch's `torchvision` package.\n",
    "2. Visualize some sample images from the dataset.\n",
    "3. Examine the distribution of different classes (digits 0-9) in the dataset.\n",
    "4. Understand basic statistics and properties of the dataset like mean, standard deviation, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the Data\n",
    "\n",
    "The MNIST dataset can be easily loaded using the `torchvision.datasets` package from PyTorch. It provides an easy-to-use API for downloading and accessing the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mnist_data_loader(transform, batch_size=32):\n",
    "    # Download and load the MNIST data\n",
    "    train_data = datasets.MNIST(\n",
    "        root=\"data\", train=True, download=True, transform=transform\n",
    "    )\n",
    "    test_data = datasets.MNIST(\n",
    "        root=\"data\", train=False, download=True, transform=transform\n",
    "    )\n",
    "    combined_data = torch.utils.data.ConcatDataset([train_data, test_data])\n",
    "    # DataLoader allows us to batch and shuffle the data\n",
    "    train_loader = DataLoader(combined_data, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "    return train_loader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The FashionMNIST dataset is a drop-in replacement for the MNIST dataset. It has the same number of training and test examples and the same image size and structure. The only difference is that the images are of fashion items rather than handwritten digits. We'll use the FashionMNIST dataset in the next notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_fashion_data_loader(transform, batch_size=32):\n",
    "    # Download and load the MNIST data\n",
    "    train_data = datasets.FashionMNIST(\n",
    "        root=\"data\", train=True, download=True, transform=transform\n",
    "    )\n",
    "    test_data = datasets.FashionMNIST(\n",
    "        root=\"data\", train=False, download=True, transform=transform\n",
    "    )\n",
    "    combined_data = torch.utils.data.ConcatDataset([train_data, test_data])\n",
    "    # DataLoader allows us to batch and shuffle the data\n",
    "    train_loader = DataLoader(combined_data, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "    # test_loader = DataLoader(test_data, batch_size=32, shuffle=False)\n",
    "    return train_loader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample Images\n",
    "\n",
    "Visualizing data samples is a powerful way to get a sense of what you're working with. This sub-section will display some example images from the MNIST and FashionMNIST datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to display an image grid\n",
    "def imshow(img):\n",
    "    img = img / 2 + 0.5  # Unnormalize the image\n",
    "    npimg = img.numpy()\n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def show_sample(loader, number_of_real_samples=32):\n",
    "    if number_of_real_samples > 32:\n",
    "        print(\"Number of samples should be less or equal than 32\")\n",
    "        number_of_real_samples = 32\n",
    "    # Get one batch of training images\n",
    "    dataiter = iter(loader)\n",
    "    images, labels = next(dataiter)\n",
    "    # Display the images in a grid\n",
    "    imshow(torchvision.utils.make_grid(images[:number_of_real_samples]))\n",
    "\n",
    "\n",
    "transformer = transforms.Compose(\n",
    "    [\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=(0.5,), std=(0.5,)),\n",
    "    ]\n",
    ")\n",
    "\n",
    "mnist_loader = get_mnist_data_loader(transform=transformer)\n",
    "fashion_loader = get_fashion_data_loader(transform=transformer)\n",
    "\n",
    "show_sample(mnist_loader)\n",
    "show_sample(fashion_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Class Distribution\n",
    "\n",
    "Understanding the distribution of different classes within the dataset is crucial. This can help us assess whether the dataset is balanced or skewed, which in turn informs model design and evaluation metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def class_distribution(loader, name):\n",
    "    # Initialize a Counter object\n",
    "    label_count = Counter()\n",
    "\n",
    "    # Loop through the dataset to count the labels\n",
    "    for _, labels in loader:\n",
    "        label_count.update(labels.numpy())\n",
    "\n",
    "    # Convert the Counter object to a dictionary for easier manipulation\n",
    "    label_count_dict = dict(label_count)\n",
    "\n",
    "    # Print the label distribution\n",
    "    print(\"Class Distribution:\", label_count_dict)\n",
    "\n",
    "    labels = list(label_count_dict.keys())\n",
    "    counts = list(label_count_dict.values())\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.bar(labels, counts)\n",
    "    plt.xlabel(\"Labels\")\n",
    "    plt.ylabel(\"Count\")\n",
    "    plt.title(f\"Class Distribution for {name} Dataset\")\n",
    "    plt.xticks(labels)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "class_distribution(mnist_loader, \"MNIST\")\n",
    "class_distribution(fashion_loader, \"Fashion MNIST\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After examining the class distribution of the MNIST dataset, it's evident that the classes are well-balanced. Each digit from 0 to 9 appears in roughly the same number of instances in the dataset. Therefore, we don't need to worry about class imbalance affecting our model's performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GAN Training Auxiliary Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### generate_fake_images\n",
    "\n",
    "- **Role**: Generates a set of fake images using the generator.\n",
    "- **Parameters**:\n",
    "  - `generator`: The generator model.\n",
    "  - `noise_size`: Size of the latent vector or noise.\n",
    "  - `number_of_fake_samples`: Number of fake images to generate (default: 8)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_fake_images(generator, noise_size, number_of_fake_samples=8):\n",
    "    with torch.no_grad():\n",
    "        # Create a batch of latent vectors (noise)\n",
    "        z = torch.randn(number_of_fake_samples, noise_size).to(device)\n",
    "        # Generate images from the latent vector z\n",
    "        fake_images = generator(z)\n",
    "        # Move the tensor from GPU to CPU and detach it from the computation graph\n",
    "        fake_images = fake_images.cpu().detach()\n",
    "        grid_images = torchvision.utils.make_grid(\n",
    "            fake_images, nrow=8\n",
    "        )  # Arrange the images into a grid with 8 columns\n",
    "        # Use the adapted imshow function\n",
    "        imshow(grid_images)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### train_discriminator\n",
    "\n",
    "- **Role**: Trains the discriminator model for K iterations.\n",
    "- **Parameters**:\n",
    "  - `discriminator`, `real_images`, `fake_images`, `criterion`, `optimizer_d`, etc.\n",
    "  - `do_smoothing`: Flag to determine if label smoothing is applied.\n",
    "  - `do_instance_noise`: Flag to determine if instance noise is added.\n",
    "\n",
    "#### Instance Noise and Label Smoothing\n",
    "\n",
    "- **Role**: Add regularization to stabilize training.\n",
    "  - `add_instance_noise`: Adds noise to real and fake images.\n",
    "  - `smooth_positive_labels`: Applies label smoothing to positive labels.\n",
    "  - `smooth_negative_labels`: Applies label smoothing to negative labels.\n",
    "- **Parameters**:\n",
    "  - For `add_instance_noise`: `std` determines the noise standard deviation.\n",
    "  - For label smoothing functions: Input tensor `y`.\n",
    "\n",
    "> While the original GAN paper did not incorporate techniques like Instance Noise and Label Smoothing, they have been introduced in subsequent research and practical implementations as ways to improve GAN stability and performance. These techniques often act as regularizers, helping to ensure smoother gradients and more robust training. In the context of this project, the inclusion of Instance Noise and Label Smoothing came as a recommendation from my tutor, and it aligns with the set of best practices compiled by the AI community for training stable GANs. For more insights and a collection of GAN hacks, you can refer to [this repository](https://github.com/soumith/ganhacks) maintained by Soumith Chintala."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_instance_noise(images, std=0.01):\n",
    "    return images + torch.randn_like(images) * std\n",
    "\n",
    "\n",
    "def smooth_positive_labels(y):\n",
    "    return y - 0.3 + (torch.rand(y.size()).to(device) * 0.3)  # 0.7 to 1.0\n",
    "\n",
    "\n",
    "def smooth_negative_labels(y):\n",
    "    return y + torch.rand(y.size()).to(device) * 0.3  # 0.0 to 0.3\n",
    "\n",
    "\n",
    "def train_discriminator(\n",
    "    discriminator,\n",
    "    real_images,\n",
    "    fake_images,\n",
    "    criterion,\n",
    "    optimizer_d,\n",
    "    k,\n",
    "    batch_size,\n",
    "    do_smoothing=False,\n",
    "    do_instance_noise=False,\n",
    "):\n",
    "    for _ in range(k):\n",
    "        # Discriminator training\n",
    "        optimizer_d.zero_grad()\n",
    "\n",
    "        if do_instance_noise:\n",
    "            real_images = add_instance_noise(real_images)\n",
    "            fake_images = add_instance_noise(fake_images)\n",
    "\n",
    "        # Real images\n",
    "        labels_real = torch.ones(real_images.size(0), 1).to(device)\n",
    "        if do_smoothing:\n",
    "            labels_real = smooth_positive_labels(labels_real)\n",
    "\n",
    "        outputs_real = discriminator(real_images)\n",
    "\n",
    "        loss_real = criterion(outputs_real, labels_real)\n",
    "\n",
    "        labels_fake = torch.zeros(batch_size, 1).to(device)\n",
    "        if do_smoothing:\n",
    "            labels_fake = smooth_negative_labels(labels_fake)\n",
    "\n",
    "        outputs_fake = discriminator(fake_images.detach())\n",
    "        loss_fake = criterion(outputs_fake, labels_fake)\n",
    "\n",
    "        # Combine losses and update Discriminator\n",
    "        loss_d = loss_real + loss_fake\n",
    "        loss_d.backward()\n",
    "        optimizer_d.step()\n",
    "\n",
    "    return loss_d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### train_generator\n",
    "\n",
    "- **Role**: Trains the generator model for one iteration.\n",
    "- **Parameters**:\n",
    "  - `discriminator`, `fake_images`, `criterion`, `optimizer_g`, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_generator(discriminator, fake_images, criterion, optimizer_g, batch_size):\n",
    "    # Generator training\n",
    "    optimizer_g.zero_grad()\n",
    "\n",
    "    outputs = discriminator(fake_images)\n",
    "    labels = torch.ones(batch_size, 1).to(device)\n",
    "\n",
    "    # Update Generator\n",
    "    loss_g = criterion(outputs, labels)\n",
    "    loss_g.backward()\n",
    "    optimizer_g.step()\n",
    "    return loss_g"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### train_model\n",
    "\n",
    "- **Role**: Manages the overall training process of both generator and discriminator.\n",
    "- **Parameters**:\n",
    "  - `discriminator`, `generator`, training dataset `train_loader`, `criterion`, optimizers, etc.\n",
    "  - `do_smoothing_label`: Flag to determine if label smoothing is applied.\n",
    "  - `do_instance_noise`: Flag to determine if instance noise is added.\n",
    "  - `log_samples`: Flag to determine if fake samples should be logged/displayed.\n",
    "  - `log_interval`: Epoch interval for logging/displaying fake samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(\n",
    "    discriminator,\n",
    "    generator,\n",
    "    train_loader,\n",
    "    criterion,\n",
    "    optimizer_d,\n",
    "    optimizer_g,\n",
    "    epochs,\n",
    "    k,\n",
    "    batch_size,\n",
    "    noise_size,\n",
    "    do_smoothing_label=False,\n",
    "    do_instance_noise=False,\n",
    "    log_samples=False,\n",
    "    log_interval=10,\n",
    "):\n",
    "    losses_d = []\n",
    "    losses_g = []\n",
    "    # Training loop\n",
    "    with tqdm(total=epochs) as pbar:\n",
    "        for epoch in range(epochs):\n",
    "            for _, (real_images, _) in enumerate(train_loader):\n",
    "                # Move tensors to the device\n",
    "                real_images = real_images.to(device)\n",
    "                noise = torch.randn(batch_size, noise_size).to(device)\n",
    "                fake_images = generator(noise).to(device)\n",
    "\n",
    "                loss_d = train_discriminator(\n",
    "                    discriminator,\n",
    "                    real_images,\n",
    "                    fake_images,\n",
    "                    criterion,\n",
    "                    optimizer_d,\n",
    "                    k,\n",
    "                    batch_size,\n",
    "                    do_smoothing=do_smoothing_label,\n",
    "                    do_instance_noise=do_instance_noise,\n",
    "                )\n",
    "                loss_g = train_generator(\n",
    "                    discriminator, fake_images, criterion, optimizer_g, batch_size\n",
    "                )\n",
    "\n",
    "            pbar.update(1)\n",
    "            pbar.set_postfix(\n",
    "                {\"discriminator_loss\": loss_d.item(), \"generator_loss\": loss_g.item()},\n",
    "                refresh=True,\n",
    "            )\n",
    "\n",
    "            losses_d.append(loss_d.item())\n",
    "            losses_g.append(loss_g.item())\n",
    "\n",
    "            if log_samples and epoch % log_interval == 0:\n",
    "                generate_fake_images(generator, noise_size)\n",
    "\n",
    "    return losses_d, losses_g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the losses\n",
    "def plot_losses(losses_d, losses_g, do_avg=False, avg_window=10):\n",
    "    if do_avg:\n",
    "        losses_d = np.convolve(losses_d, np.ones(avg_window), \"valid\") / avg_window\n",
    "        losses_g = np.convolve(losses_g, np.ones(avg_window), \"valid\") / avg_window\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(losses_d, label=\"Discriminator loss\")\n",
    "    plt.plot(losses_g, label=\"Generator loss\")\n",
    "    plt.legend()\n",
    "    plt.title(\"Losses\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BaseLine Solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building a Baseline Discriminator Model\n",
    "\n",
    "#### Architecture\n",
    "\n",
    "The Discriminator is a neural network that aims to classify whether a given image is real (from the dataset) or fake (generated by the Generator). In this enhanced model, we'll incorporate Convolutional Neural Network (CNN) layers to better capture the hierarchical features in images. The updated architecture will consist of the following layers:\n",
    "\n",
    "1. **Convolutional Layer 1**: A 2D convolutional layer with 64 filters and a kernel size of 3x3, followed by a ReLU activation function.\n",
    "2. **Convolutional Layer 2**: Another 2D convolutional layer with 128 filters and a kernel size of 3x3, followed by a ReLU activation function.\n",
    "3. **Flatten**: A layer to flatten the output from the convolutional layers.\n",
    "4. **Fully Connected Layers**: One hidden layer with 128 neurons, using a ReLU activation function.\n",
    "5. **Output Layer**: A single neuron with a Sigmoid activation function to output the probability that the input image is real.\n",
    "\n",
    "#### Objective and Loss Function\n",
    "\n",
    "The objective of the Discriminator is derived from a [minimax](https://en.wikipedia.org/wiki/Minimax) two-player game as defined in the original [GAN paper](https://arxiv.org/pdf/1406.2661.pdf). The value function $V(D, G)$ for this game is:\n",
    "\n",
    "$$ \n",
    "\\min_G \\max_D V(D, G) = \\mathbb{E}_{x \\sim p_{\\text{data}}(x)}[\\log D(x)] + \\mathbb{E}_{z \\sim p_{z}(z)}[\\log(1 - D(G(z)))] \n",
    "$$\n",
    "\n",
    "This can be interpreted as maximizing the log-likelihood of correctly classifying both real and fake samples. In practical terms, this objective is implemented using binary cross-entropy loss, with the Discriminator aiming to output values close to 1 for real images and close to 0 for fake images.\n",
    "\n",
    "#### Implementation\n",
    "\n",
    "The model will be implemented using PyTorch. Given the objective and the architecture, the Discriminator will be trained using real images labeled as 1 and fake images labeled as 0. The inclusion of convolutional layers aims to improve its ability to distinguish between real and fake images effectively.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseLineDiscriminator(nn.Module):\n",
    "    def __init__(self, in_channels, img_size):\n",
    "        super(BaseLineDiscriminator, self).__init__()\n",
    "\n",
    "        # Convolutional layers\n",
    "        self.conv1 = nn.Conv2d(in_channels, 64, kernel_size=3, stride=2, padding=1)\n",
    "        self.conv2 = nn.Conv2d(64, 128, kernel_size=3, stride=2, padding=1)\n",
    "\n",
    "        # Calculate the output dimension after conv and pooling layers\n",
    "        self.calc_dim = lambda x: int(((x - 2) // 2 + 1) // 2)\n",
    "        self.flat_dim = (\n",
    "            128\n",
    "            * self.calc_dim(self.calc_dim(img_size))\n",
    "            * self.calc_dim(self.calc_dim(img_size))\n",
    "        )\n",
    "\n",
    "        # Fully connected layers\n",
    "        self.fc1 = nn.Linear(self.flat_dim, 128)\n",
    "        self.fc2 = nn.Linear(128, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.max_pool2d(x, 2, 2)  # 2x2 max pooling\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.max_pool2d(x, 2, 2)  # 2x2 max pooling\n",
    "        x = x.view(-1, self.flat_dim)  # flatten\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = torch.sigmoid(self.fc2(x))  # output between 0 and 1\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building a Baseline Generator Model\n",
    "\n",
    "#### Architecture\n",
    "\n",
    "The Generator is a neural network designed to create synthetic data that resembles a real dataset. For image generation tasks, it employs a series of Transposed Convolutional Neural Network (DeconvNet) layers that help upscale a lower-dimensional input to a full-sized image. The architecture consists of the following layers:\n",
    "\n",
    "1. **Fully Connected Layer**: Takes a noise vector (e.g., 100-dimensional) as input and maps it to a higher-dimensional space (256 * 7 * 7), setting the stage for the transposed convolutions.\n",
    "2. **Transposed Convolutional Layer 1**: A 2D transposed convolutional layer with 128 filters and a kernel size of 3x3. It is followed by a ReLU activation function.\n",
    "3. **Transposed Convolutional Layer 2**: Another 2D transposed convolutional layer with 64 filters and a kernel size of 3x3, also followed by a ReLU activation function.\n",
    "4. **Output Layer**: A final 2D transposed convolutional layer with a single filter, having a kernel size of 3x3, with a Tanh activation function to output an image in a range between -1 and 1.\n",
    "\n",
    "#### Objective and Loss Function\n",
    "\n",
    "The objective of the Generator, in the context of a Generative Adversarial Network (GAN), is to generate data that is indistinguishable from real data by the Discriminator. Mathematically, the Generator aims to minimize the following function in a [minimax game](https://en.wikipedia.org/wiki/Minimax) against the Discriminator:\n",
    "\n",
    "$$\n",
    "\\min_G \\max_D V(D, G) = \\mathbb{E}_{x \\sim p_{\\text{data}}(x)}[\\log D(x)] + \\mathbb{E}_{z \\sim p_{z}(z)}[\\log(1 - D(G(z)))]\n",
    "$$\n",
    "\n",
    "In simple terms, the Generator wants to maximize the error of the Discriminator in classifying its output as fake. The loss function generally used is the binary cross-entropy loss.\n",
    "\n",
    "#### Implementation\n",
    "\n",
    "The Generator model is implemented using PyTorch. The architecture is built to transform random noise vectors into images that should ideally fool the Discriminator into thinking they are real. The sequence of transposed convolutional layers helps in this upscaling and feature refinement to generate high-quality images.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseLineGenerator(nn.Module):\n",
    "    def __init__(self, noise_size, out_channels, img_size):\n",
    "        super(BaseLineGenerator, self).__init__()\n",
    "\n",
    "        # Calculate dimensions to back out to the image size\n",
    "        self.init_height = img_size // 4\n",
    "        self.init_width = img_size // 4\n",
    "\n",
    "        # Fully connected layer to reshape the input noise vector\n",
    "        self.fc = nn.Linear(noise_size, 256 * self.init_height * self.init_width)\n",
    "\n",
    "        # Transposed convolutional layers\n",
    "        self.deconv1 = nn.ConvTranspose2d(\n",
    "            256, 128, kernel_size=3, stride=2, padding=1, output_padding=1\n",
    "        )\n",
    "        self.deconv2 = nn.ConvTranspose2d(\n",
    "            128, 64, kernel_size=3, stride=2, padding=1, output_padding=1\n",
    "        )\n",
    "        self.deconv3 = nn.ConvTranspose2d(\n",
    "            64, out_channels, kernel_size=3, stride=1, padding=1\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc(x)\n",
    "        x = x.view(\n",
    "            -1, 256, self.init_height, self.init_width\n",
    "        )  # Reshape to (batch_size, num_channels, width, height)\n",
    "        x = F.relu(self.deconv1(x))\n",
    "        x = F.relu(self.deconv2(x))\n",
    "        x = torch.tanh(self.deconv3(x))  # output between -1 and 1\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mnist (Baseline)\n",
    "\n",
    "In this segment, we set out to train a Generative Adversarial Network (GAN) on the MNIST dataset. We initiate with defining key hyperparameters like learning rates, batch size, and epochs. Both the Discriminator and Generator architectures are instantiated using predefined classes, while leveraging the Binary Cross Entropy loss. Optimizers for each are chosen from the Adam family. The MNIST data undergoes transformations to fit our model requirements. Subsequently, the model is trained, incorporating best practices like label smoothing and instance noise. Finally, the loss trend for both Generator and Discriminator is visualized, offering insights into the GAN's performance trajectory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "learning_rate_d = 0.0001\n",
    "learning_rate_g = 0.0001\n",
    "batch_size = 64\n",
    "epochs = 50\n",
    "noise_size = 100  # Dimension of random noise vector for Generator\n",
    "k = 2  # Number of steps to apply to the Discriminator\n",
    "img_size = 32  # Size of the image\n",
    "\n",
    "# Create the Discriminator model\n",
    "discriminator = BaseLineDiscriminator(1, img_size).to(device)\n",
    "# Create the Generator model\n",
    "generator = BaseLineGenerator(noise_size, 1, img_size).to(device)\n",
    "\n",
    "# Loss and optimizers\n",
    "criterion = (\n",
    "    nn.BCELoss()\n",
    ")  # Binary cross entropy loss https://pytorch.org/docs/stable/generated/torch.nn.BCELoss.html\n",
    "optimizer_d = torch.optim.Adam(discriminator.parameters(), lr=learning_rate_d)\n",
    "optimizer_g = torch.optim.Adam(generator.parameters(), lr=learning_rate_g)\n",
    "\n",
    "# Get the data loader\n",
    "transform = transforms.Compose(\n",
    "    [\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Resize((img_size, img_size)),\n",
    "        transforms.Normalize((0.5,), (0.5,)),\n",
    "    ]\n",
    ")\n",
    "train_loader = get_mnist_data_loader(transform, batch_size)\n",
    "\n",
    "# Train the model\n",
    "losses_d, losses_g = train_model(\n",
    "    discriminator,\n",
    "    generator,\n",
    "    train_loader,\n",
    "    criterion,\n",
    "    optimizer_d,\n",
    "    optimizer_g,\n",
    "    epochs,\n",
    "    k,\n",
    "    batch_size,\n",
    "    noise_size,\n",
    "    do_smoothing_label=True,\n",
    "    do_instance_noise=True,\n",
    "    log_samples=True,\n",
    "    log_interval=10,\n",
    ")\n",
    "plot_losses(losses_d, losses_g, do_avg=True, avg_window=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Observations from Loss Graph\n",
    "- **Initial Spike**: The discriminator's loss starts very high but quickly settles.\n",
    "- **Oscillations**: Both the generator and discriminator loss oscillate, which is typical in GANs, signifying the ongoing \"battle\" between the two.\n",
    "- **Trend**: As training progresses, both losses stabilize but still retain their oscillatory behavior."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Results Analysis\n",
    "\n",
    "##### Overview\n",
    "After training the Generative Adversarial Network on the MNIST dataset, we generated a set of images to compare with the actual MNIST samples. The aim is to visually assess the quality of the generated images and identify any patterns or discrepancies.\n",
    "\n",
    "##### Generated Images vs. Real Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_fake_images(generator, noise_size, 32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_sample(mnist_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Observations\n",
    "\n",
    "1. **Quality & Resemblance**: The generated images, at a glance, bear a strong resemblance to the real MNIST samples. The digits in the generated images have distinct shapes and structures.\n",
    "\n",
    "2. **Mode Collapse**: One observation from the generated samples is a phenomenon known as mode collapse. This is where the generator produces limited varieties of samples, or in some cases, even just one type of sample. In our generated images, there are certain numbers that appear more frequently while others are underrepresented or even missing.\n",
    "\n",
    "3. **Metrics & Evaluation**: While visual comparison provides insight into the quality of the generated images, there are metrics designed to evaluate the distribution, diversity, and fidelity of generated images, such as the Inception Score or the Frechet Inception Distance. However, the scope of this project is limited to a visual comparison between real and generated images.\n",
    "\n",
    "##### Conclusion\n",
    "Visual inspection suggests that our GAN has done a commendable job at generating images that resemble the MNIST dataset. However, the mode collapse observed indicates room for improvement in the training process or model architecture. While metrics exist to provide a more quantifiable evaluation, our assessment remains qualitative, focusing on direct visual comparison."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fashion MNIST (Baseline)\n",
    "\n",
    "In this exploration, we venture into training a Generative Adversarial Network (GAN) on the Fashion MNIST dataset, a collection of clothing images belonging to various categories. The process kicks off with the establishment of pivotal hyperparameters including learning rates, batch size, and the total number of epochs. The Discriminator and Generator modules are sculpted using predetermined classes, and the Binary Cross Entropy loss is designated as our chief criterion. The optimizers employed for both the Generator and Discriminator hail from the Adam lineage. As we delve deeper, the Fashion MNIST dataset undergoes meticulous transformations to align with our model's specifications. Upon setting the stage, the GAN training commences, enriched with best practices such as label smoothing and instance noise. In culmination, we cast a spotlight on the loss trends for both the Generator and Discriminator, which unfurl the GAN's evolutionary path and render insights into its progression.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate_d = 0.0001\n",
    "learning_rate_g = 0.0001\n",
    "batch_size = 64\n",
    "epochs = 100\n",
    "noise_size = 100  # Dimension of random noise vector for Generator\n",
    "k = 2  # Number of steps to apply to the Discriminator\n",
    "img_size = 32  # Size of the image\n",
    "\n",
    "# Create the Discriminator model\n",
    "discriminator = BaseLineDiscriminator(1, img_size).to(device)\n",
    "# Create the Generator model\n",
    "generator = BaseLineGenerator(noise_size, 1, img_size).to(device)\n",
    "\n",
    "# Loss and optimizers\n",
    "criterion = (\n",
    "    nn.BCELoss()\n",
    ")  # Binary cross entropy loss https://pytorch.org/docs/stable/generated/torch.nn.BCELoss.html\n",
    "optimizer_d = torch.optim.Adam(discriminator.parameters(), lr=learning_rate_d)\n",
    "optimizer_g = torch.optim.Adam(generator.parameters(), lr=learning_rate_g)\n",
    "\n",
    "# Get the data loader\n",
    "transform = transforms.Compose(\n",
    "    [\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Resize((img_size, img_size)),\n",
    "        transforms.Normalize((0.5,), (0.5,)),\n",
    "    ]\n",
    ")\n",
    "train_loader = get_fashion_data_loader(transform, batch_size)\n",
    "\n",
    "# Train the model\n",
    "losses_d, losses_g = train_model(\n",
    "    discriminator,\n",
    "    generator,\n",
    "    train_loader,\n",
    "    criterion,\n",
    "    optimizer_d,\n",
    "    optimizer_g,\n",
    "    epochs,\n",
    "    k,\n",
    "    batch_size,\n",
    "    noise_size,\n",
    "    do_smoothing_label=True,\n",
    "    do_instance_noise=True,\n",
    "    log_samples=True,\n",
    "    log_interval=20,\n",
    ")\n",
    "plot_losses(losses_d, losses_g, True, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Observations from Loss Graph\n",
    "- **Initial Fluctuations:** Early stages show significant fluctuation, common in GANs.\n",
    "- **Discriminator Loss:** After the initial epochs, there's a downtrend with some spikes, indicating challenges faced against the Generator.\n",
    "- **Generator Loss:** A general upward trend post the initial stages. This suggests increasing challenges in generating convincing images.\n",
    "- **Convergence Issues:** Neither loss shows stable convergence, hinting at potential challenges like mode collapse.\n",
    "- **Late Epochs:** The diverging losses towards the end may necessitate hyperparameter tuning or alternate training strategies."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Results Analysis\n",
    "\n",
    "##### Generated Images vs. Real Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_fake_images(generator, noise_size, 32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_sample(train_loader, 32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Observations\n",
    "\n",
    "1. **Generated Quality:** The generated images from the GAN, although recognizable, exhibit certain noise and lack the sharpness observed in real images.\n",
    "\n",
    "2. **Item Diversity:** The Generator has managed to produce a diverse set of fashion items, from tops to shoes. This is encouraging as it indicates the Generator isn't stuck on one mode.\n",
    "\n",
    "3. **Detailing:** Close examination reveals that while the broader shapes are captured, finer details (like patterns on clothes) are either distorted or missing in generated images.\n",
    "\n",
    "4. **Challenges:** Some generated images show artifacts, hinting that the Generator still has room for improvement. This may be addressed with further training or tweaking."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experimentation 1: Exploring Advanced Models\n",
    "\n",
    "As we continue our journey in the vast world of Generative Adversarial Networks (GANs), we recognize the importance of model architecture in influencing outcomes. While baseline models have provided valuable insights, there's an entire spectrum of advanced architectures waiting to be tapped into.\n",
    "\n",
    "In this experimentation phase, we set our sights on leveraging more intricate designs for both our Discriminator and Generator. Specifically:\n",
    "\n",
    "- **Discriminator:** We'll be venturing into the realm of DenseNets. Known for their dense connections, these networks can offer a rich gradient flow, potentially amplifying the power of our Discriminator.\n",
    "\n",
    "- **Generator:** Two intriguing architectures come into play here:\n",
    "  - **ResidualGenerator:** Drawing inspiration from residual blocks, this design aims to capture intricate patterns while maintaining a smooth gradient flow.\n",
    "  - **UpSampleGenerator:** This is rooted in the concept of UpSample blocks, which can progressively refine generated images, making them more detailed and crisp.\n",
    "\n",
    "As we set the stage for these experiments, remember that our focus remains on improving the overall quality and diversity of generated images. It's an exciting phase of exploration, promising new insights and possibilities. Let the experiments begin!\n",
    "\n",
    "> **Note:** While we are exploring more intricate and complex models, it's essential to understand that complexity doesn't always translate to better results. Each dataset and use-case has its nuances, and sometimes simpler architectures might outperform their complex counterparts. It's all about finding the right balance and fit for the task at hand.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DenseNet Discriminator Overview\n",
    "\n",
    "The provided model appears to be a [DenseNet](https://arxiv.org/abs/1608.06993)-based discriminator, typically used within Generative Adversarial Networks (GANs). The architecture can be described at a high level as follows:\n",
    "\n",
    "#### 1. **ConvLayer**\n",
    "- This is a basic building block that performs a 2D convolution, followed by a [batch normalization](https://github.com/soumith/ganhacks#4-batchnorm) and a [leaky ReLU activation](https://github.com/soumith/ganhacks#5-avoid-sparse-gradients-relu-maxpool).\n",
    "\n",
    "#### 2. **DenseBlock**\n",
    "- The heart of the DenseNet architecture. In this block, the output from each layer is concatenated and used as an input for subsequent layers.\n",
    "- It consists of multiple `ConvLayer` units.\n",
    "\n",
    "#### 3. **TransitionLayer**\n",
    "- This layer is used to downsample the spatial dimensions of the feature maps and to reduce the number of channels.\n",
    "- It consists of a `ConvLayer` followed by an average pooling operation.\n",
    "\n",
    "#### 4. **DenseNetDiscriminator**\n",
    "- The main architecture which houses the entire flow of the network.\n",
    "- It starts with an initial convolution followed by max pooling.\n",
    "- Then it consists of a series of `DenseBlock` and `TransitionLayer` units, iterating through dense layers and transition layers.\n",
    "- Finally, the feature maps are pooled globally, and passed through two fully connected layers to output a single value after the sigmoid activation.\n",
    "\n",
    "This architecture leverages the power of DenseNet to capture features at various levels and scales for discriminating real from generated images.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvLayer(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size=3, stride=1, padding=1):\n",
    "        super(ConvLayer, self).__init__()\n",
    "        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding)\n",
    "        self.bn = nn.BatchNorm2d(out_channels)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = self.bn(x)\n",
    "        x = F.leaky_relu(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class DenseBlock(nn.Module):\n",
    "    def __init__(self, in_channels, growth_rate, num_layers):\n",
    "        super(DenseBlock, self).__init__()\n",
    "        self.layers = nn.ModuleList()\n",
    "        for i in range(num_layers):\n",
    "            self.layers.append(ConvLayer(in_channels + i * growth_rate, growth_rate))\n",
    "\n",
    "    def forward(self, x):\n",
    "        outputs = [x]\n",
    "        for layer in self.layers:\n",
    "            out = layer(torch.cat(outputs, dim=1))\n",
    "            outputs.append(out)\n",
    "        return torch.cat(outputs, dim=1)\n",
    "\n",
    "\n",
    "class TransitionLayer(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(TransitionLayer, self).__init__()\n",
    "        self.conv = ConvLayer(\n",
    "            in_channels, out_channels, kernel_size=1, stride=1, padding=0\n",
    "        )\n",
    "        self.pool = nn.AvgPool2d(2, stride=2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = self.pool(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class DenseNetDiscriminator(nn.Module):\n",
    "    def __init__(self, in_channels, img_size):\n",
    "        super(DenseNetDiscriminator, self).__init__()\n",
    "        self.input_size = img_size\n",
    "        self.init_conv = ConvLayer(in_channels, 64, kernel_size=7, stride=2, padding=3)\n",
    "        self.pool = nn.MaxPool2d(3, stride=2, padding=1)\n",
    "\n",
    "        self.dense1 = DenseBlock(64, 32, 4)\n",
    "        self.trans1 = TransitionLayer(192, 96)  # 64 + 4 * 32 = 192 | 192 / 2 = 96\n",
    "        self.dense2 = DenseBlock(96, 32, 4)\n",
    "        self.trans2 = TransitionLayer(224, 112)  # 96 + 4 * 32 = 224 | 224 / 2 = 112\n",
    "        self.dense3 = DenseBlock(112, 32, 4)\n",
    "        self.trans3 = TransitionLayer(240, 120)  # 112 + 4 * 32 = 240 | 240 / 2 = 120\n",
    "        self.dense4 = DenseBlock(120, 32, 4)\n",
    "\n",
    "        self.fc = nn.Linear(248, 512)  # 120 + 4 * 32 = 248\n",
    "        self.fc2 = nn.Linear(512, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.init_conv(x)\n",
    "        x = self.pool(x)\n",
    "\n",
    "        x = self.dense1(x)\n",
    "        x = self.trans1(x)\n",
    "        x = self.dense2(x)\n",
    "        x = self.trans2(x)\n",
    "        x = self.dense3(x)\n",
    "        x = self.trans3(x)\n",
    "        x = self.dense4(x)\n",
    "\n",
    "        x = F.adaptive_avg_pool2d(x, (1, 1))\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = F.leaky_relu(self.fc(x))\n",
    "        x = self.fc2(x)\n",
    "        x = torch.sigmoid(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Residual Generator Overview\n",
    "\n",
    "The architecture presented is a type of generative network that makes use of residual blocks, which have become popular due to their effectiveness in deep learning models, particularly the [ResNet](https://arxiv.org/abs/1512.03385) architecture.\n",
    "\n",
    "#### 1. **ResidualBlock**\n",
    "- **Primary Components**: Two convolutional layers, each accompanied by a [batch normalization]((https://github.com/soumith/ganhacks#4-batchnorm)) layer.\n",
    "- **Function**: Each of the two convolution operations is followed by a batch normalization and a [leaky ReLU activation](https://github.com/soumith/ganhacks#5-avoid-sparse-gradients-relu-maxpool) function. If there's a change in dimensions, a shortcut connection is established to match the output shape.\n",
    "\n",
    "#### 2. **ResidualGenerator**\n",
    "- **Initialization**: It calculates the initial dimensions required to match the final image size. A fully connected layer is then used to reshape the input noise vector.\n",
    "  \n",
    "- **Deconvolutional Layers**:\n",
    "  - These layers are used to upscale feature maps.\n",
    "  - The first deconvolution is followed by a batch normalization and a leaky ReLU activation.\n",
    "  \n",
    "- **Residual Block Integration**: After the first deconvolution, a residual block is applied, providing the network an ability to learn identity functions that help in training deeper models.\n",
    "  \n",
    "- **Dropout**: This is used after the residual block to prevent overfitting and provide a form of regularization ([link](https://arxiv.org/pdf/1611.07004v1.pdf)).\n",
    "  \n",
    "- **Final Layers**: Two more deconvolutional layers are used to further upsample the feature maps. After the final deconvolution, a tanh activation function ensures the output values are between -1 and 1, making it suitable for image data.\n",
    "\n",
    "The architecture utilizes the power of residual blocks, which helps in avoiding the vanishing gradient problem, thereby allowing for deeper and more effective generative models.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Residual Block\n",
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, stride=1):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "\n",
    "        # First convolutional layer in the residual block\n",
    "        # Followed by Batch Normalization\n",
    "        self.conv1 = nn.Conv2d(\n",
    "            in_channels,\n",
    "            out_channels,\n",
    "            kernel_size=3,\n",
    "            stride=stride,\n",
    "            padding=1,\n",
    "            bias=False,\n",
    "        )\n",
    "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
    "\n",
    "        # Second convolutional layer in the residual block\n",
    "        # Followed by Batch Normalization\n",
    "        self.conv2 = nn.Conv2d(\n",
    "            out_channels, out_channels, kernel_size=3, stride=1, padding=1, bias=False\n",
    "        )\n",
    "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
    "\n",
    "        # Shortcut connection to match dimensions\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_channels != out_channels:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(\n",
    "                    in_channels, out_channels, kernel_size=1, stride=stride, bias=False\n",
    "                ),\n",
    "                nn.BatchNorm2d(out_channels),\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Forward pass\n",
    "        out = F.leaky_relu(self.bn1(self.conv1(x)))  # First Conv -> BN -> ReLU\n",
    "        out = self.bn2(self.conv2(out))  # Second Conv -> BN\n",
    "        out += self.shortcut(x)  # Add the shortcut\n",
    "        out = F.leaky_relu(out)  # Final ReLU\n",
    "        return out\n",
    "\n",
    "\n",
    "class ResidualGenerator(nn.Module):\n",
    "    def __init__(self, noise_size, out_channels, img_size):\n",
    "        super(ResidualGenerator, self).__init__()\n",
    "\n",
    "        # Calculate dimensions to back out to the image size\n",
    "        self.init_height = img_size // 4\n",
    "        self.init_width = img_size // 4\n",
    "\n",
    "        # Fully connected layer to reshape the input noise vector\n",
    "        self.fc = nn.Linear(noise_size, 256 * self.init_height * self.init_width)\n",
    "\n",
    "        # Transposed convolutional layers\n",
    "        self.deconv1 = nn.ConvTranspose2d(\n",
    "            256, 128, kernel_size=3, stride=2, padding=1, output_padding=1\n",
    "        )\n",
    "        self.bn1 = nn.BatchNorm2d(128)\n",
    "\n",
    "        # Add a Residual Block\n",
    "        self.res_block = ResidualBlock(128, 128)\n",
    "\n",
    "        # Add Dropout\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "\n",
    "        self.deconv2 = nn.ConvTranspose2d(\n",
    "            128, 64, kernel_size=3, stride=2, padding=1, output_padding=1\n",
    "        )\n",
    "        self.bn2 = nn.BatchNorm2d(64)\n",
    "        self.deconv3 = nn.ConvTranspose2d(\n",
    "            64, out_channels, kernel_size=3, stride=1, padding=1\n",
    "        )\n",
    "        self.bn3 = nn.BatchNorm2d(out_channels)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc(x)\n",
    "\n",
    "        x = x.view(\n",
    "            -1, 256, self.init_height, self.init_width\n",
    "        )  # Reshape to (batch_size, num_channels, width, height)\n",
    "\n",
    "        x = self.deconv1(x)\n",
    "        x = self.bn1(x)  # BatchNorm\n",
    "        x = F.leaky_relu(x)\n",
    "\n",
    "        x = self.res_block(x)  # Using Residual Block\n",
    "        x = self.dropout(x)  # Applying Dropout\n",
    "\n",
    "        x = self.deconv2(x)\n",
    "        x = self.bn2(x)  # BatchNorm\n",
    "        x = F.leaky_relu(x)\n",
    "\n",
    "        x = self.deconv3(x)\n",
    "        x = self.bn3(x)\n",
    "\n",
    "        x = torch.tanh(x)  # output between -1 and 1\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mnist (DenseNet Discriminator vs ResNet Generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "learning_rate_d = 0.00005\n",
    "learning_rate_g = 0.0001\n",
    "batch_size = 64\n",
    "epochs = 50\n",
    "noise_size = 100  # Dimension of random noise vector for Generator\n",
    "k = 5  # Number of steps to apply to the Discriminator\n",
    "img_size = 32  # Size of the image\n",
    "\n",
    "# Create the Discriminator model\n",
    "discriminator = DenseNetDiscriminator(1, img_size).to(device)\n",
    "# Create the Generator model\n",
    "generator = ResidualGenerator(noise_size, 1, img_size).to(device)\n",
    "\n",
    "# Loss and optimizers\n",
    "criterion = (\n",
    "    nn.BCELoss()\n",
    ")  # Binary cross entropy loss https://pytorch.org/docs/stable/generated/torch.nn.BCELoss.html\n",
    "optimizer_d = torch.optim.SGD(discriminator.parameters(), lr=learning_rate_d)\n",
    "optimizer_g = torch.optim.Adam(generator.parameters(), lr=learning_rate_g)\n",
    "\n",
    "# Get the data loader\n",
    "transform = transforms.Compose(\n",
    "    [\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Resize((img_size, img_size)),\n",
    "        transforms.Normalize((0.5,), (0.5,)),\n",
    "    ]\n",
    ")\n",
    "train_loader = get_mnist_data_loader(transform, batch_size)\n",
    "\n",
    "# Train the model\n",
    "losses_d, losses_g = train_model(\n",
    "    discriminator,\n",
    "    generator,\n",
    "    train_loader,\n",
    "    criterion,\n",
    "    optimizer_d,\n",
    "    optimizer_g,\n",
    "    epochs,\n",
    "    k,\n",
    "    batch_size,\n",
    "    noise_size,\n",
    "    do_smoothing_label=True,\n",
    "    do_instance_noise=True,\n",
    "    log_samples=True,\n",
    "    log_interval=10,\n",
    ")\n",
    "plot_losses(losses_d, losses_g, True, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Observations from Loss Graph\n",
    "\n",
    "1. **Quick Discriminator Adaptation**: The initial high loss suggests the discriminator's early struggles, but its swift decline indicates rapid learning.\n",
    "2. **Tug-of-War Dynamics**: Oscillatory losses highlight the ongoing \"battle\" between the generator and discriminator.\n",
    "3. **Potential Equilibrium**: The stabilizing losses hint at a near-equilibrium state. However, this could also indicate possible mode collapse or being stuck in a local minimum.\n",
    "\n",
    "It's vital to inspect generated images alongside loss values for a comprehensive assessment.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_fake_images(generator, noise_size, 32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_sample(mnist_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Comments\n",
    "\n",
    "- **Complex Model Architecture**: Managing both the discriminator and generator was intricate. The nuanced interplay between them presented a substantial training challenge.\n",
    "- **Sensitive to Hyperparameters**: Even a slight change in the learning rate drastically affected training. In some cases, the generator failed to produce meaningful images after just a few epochs.\n",
    "- **Stable Training Without Mode Collapse**: The stabilizing losses, combined with the diverse generated images, suggest successful training without the pitfalls of mode collapse."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experimentation 2: Conditional GAN (cGAN) + Wasserstein GAN (WGAN)\n",
    "\n",
    "The Conditional Generative Adversarial Network (Conditional GAN or cGAN) extends the traditional GAN framework by adding conditional information to both the generator and discriminator. This approach allows for targeted generation of data based on specific conditions, granting the model a greater degree of control in data synthesis.\n",
    "\n",
    "In a high-level overview:\n",
    "- **Purpose**: While standard GANs generate outputs from random noise, cGANs produce outputs based on a given condition or label. This enables the generation of specific types of data.\n",
    "- **Mechanism**: The generator receives both random noise and a condition label to produce a sample. The discriminator then evaluates the authenticity of the sample based on the combined input of the generated sample and the condition.\n",
    "- **Applications**: cGANs have broad applications, from generating specific types of images in computer vision tasks to creating particular styles of music or specific textual content.\n",
    "\n",
    "In essence, Conditional GANs offer a refined approach to data generation, ensuring the generated content aligns more closely with desired attributes or characteristics.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ConditionalDiscriminator Overview\n",
    "\n",
    "In essence, the `ConditionalDiscriminator` incorporates the conditioning labels into its architecture, making it capable of discerning not just the authenticity of the image but also its conformity to the given label or condition. On the other hand, the `BaseLineDiscriminator` solely focuses on the authenticity of the image without any conditional context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConditionalDiscriminator(nn.Module):\n",
    "    def __init__(self, in_channels, img_size, num_classes):\n",
    "        super(ConditionalDiscriminator, self).__init__()\n",
    "\n",
    "        # Convolutional layers\n",
    "        self.conv1 = nn.Conv2d(in_channels, 64, kernel_size=3, stride=2, padding=1)\n",
    "        self.conv2 = nn.Conv2d(64, 128, kernel_size=3, stride=2, padding=1)\n",
    "\n",
    "        # Calculate the output dimension after conv and pooling layers\n",
    "        self.calc_dim = lambda x: int(((x - 2) // 2 + 1) // 2)\n",
    "        self.flat_dim = (\n",
    "            128\n",
    "            * self.calc_dim(self.calc_dim(img_size))\n",
    "            * self.calc_dim(self.calc_dim(img_size))\n",
    "        )\n",
    "\n",
    "        # Embedding for labels\n",
    "        self.label_embedding = nn.Embedding(num_classes, self.flat_dim)\n",
    "\n",
    "        # Fully connected layers\n",
    "        self.fc1 = nn.Linear(self.flat_dim * 2, 128)  # x2 because of concatenation\n",
    "        self.fc2 = nn.Linear(128, 1)\n",
    "\n",
    "    def forward(self, x, labels):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.max_pool2d(x, 2, 2)  # 2x2 max pooling\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.max_pool2d(x, 2, 2)  # 2x2 max pooling\n",
    "        x = x.view(-1, self.flat_dim)  # flatten\n",
    "\n",
    "        # Embed labels and concatenate with image features\n",
    "        labels = self.label_embedding(labels)\n",
    "        x = torch.cat([x, labels], dim=1)\n",
    "\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = torch.sigmoid(self.fc2(x))  # output between 0 and 1\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ConditionalGenerator Overview\n",
    "\n",
    "The `ConditionalGenerator` model, like its discriminator counterpart, has been designed to take into account conditioning labels when generating images. This conditioning approach allows for more controlled image generation, catering to specific classes or types based on the input labels. Conversely, the `BaseLineGenerator` lacks this conditioning capability and generates images solely based on the noise input.\n",
    "\n",
    "By introducing label conditioning into the `ConditionalGenerator`, the generated images can better adhere to specific classes or characteristics defined by the labels, enabling a more targeted and controlled generation process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConditionalGenerator(nn.Module):\n",
    "    def __init__(self, noise_size, out_channels, img_size, num_classes):\n",
    "        super(ConditionalGenerator, self).__init__()\n",
    "\n",
    "        # Calculate dimensions to back out to the image size\n",
    "        self.init_height = img_size // 4\n",
    "        self.init_width = img_size // 4\n",
    "\n",
    "        # Embedding for labels\n",
    "        self.label_embedding = nn.Embedding(num_classes, noise_size)\n",
    "\n",
    "        # Fully connected layer to reshape the input noise vector\n",
    "        self.fc = nn.Linear(\n",
    "            noise_size * 2, 256 * self.init_height * self.init_width\n",
    "        )  # x2 because of concatenation\n",
    "\n",
    "        # Transposed convolutional layers\n",
    "        self.deconv1 = nn.ConvTranspose2d(\n",
    "            256, 128, kernel_size=3, stride=2, padding=1, output_padding=1\n",
    "        )\n",
    "        self.deconv2 = nn.ConvTranspose2d(\n",
    "            128, 64, kernel_size=3, stride=2, padding=1, output_padding=1\n",
    "        )\n",
    "        self.deconv3 = nn.ConvTranspose2d(\n",
    "            64, out_channels, kernel_size=3, stride=1, padding=1\n",
    "        )\n",
    "\n",
    "    def forward(self, z, labels):\n",
    "        # Embed labels and concatenate with noise\n",
    "        labels = self.label_embedding(labels)\n",
    "        z = torch.cat([z, labels], dim=1)\n",
    "\n",
    "        x = self.fc(z)\n",
    "        x = x.view(\n",
    "            -1, 256, self.init_height, self.init_width\n",
    "        )  # Reshape to (batch_size, num_channels, width, height)\n",
    "        x = F.relu(self.deconv1(x))\n",
    "        x = F.relu(self.deconv2(x))\n",
    "        x = torch.tanh(self.deconv3(x))  # output between -1 and 1\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conditional GAN Training Process Focus\n",
    "\n",
    "A Conditional Generative Adversarial Network (CGAN) makes use of additional conditioning information to guide the generation process. This conditioning typically comes in the form of labels. Here's an in-depth look at how conditioning is integrated into the training process you provided:\n",
    "\n",
    "Both the generator and the discriminator are modified to accept labels, ensuring that the generated images aren't just realistic but also align with the given conditions. This makes CGANs powerful tools for generating images that adhere to specific requirements or classifications.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_conditional_fake_images(generator, noise_size, num_classes, number_of_fake_samples=8, labels=None):\n",
    "    with torch.no_grad():\n",
    "        # Create a batch of latent vectors (noise)\n",
    "        z = torch.randn(number_of_fake_samples, noise_size).to(device)\n",
    "        if labels is None:\n",
    "            labels = torch.randint(0, num_classes, (number_of_fake_samples,)).to(device)\n",
    "        # Generate images from the latent vector z\n",
    "        fake_images = generator(z, labels)\n",
    "        # Move the tensor from GPU to CPU and detach it from the computation graph\n",
    "        fake_images = fake_images.cpu().detach()\n",
    "        grid_images = torchvision.utils.make_grid(\n",
    "            fake_images, nrow=8\n",
    "        )\n",
    "        imshow(grid_images)\n",
    "\n",
    "def train_conditional_discriminator(\n",
    "    discriminator,\n",
    "    real_images,\n",
    "    labels,\n",
    "    fake_images,\n",
    "    criterion,\n",
    "    optimizer_d,\n",
    "    k,\n",
    "    batch_size,\n",
    "    do_smoothing=False,\n",
    "    do_instance_noise=False,\n",
    "):\n",
    "    for _ in range(k):\n",
    "        # Discriminator training\n",
    "        optimizer_d.zero_grad()\n",
    "\n",
    "        if do_instance_noise:\n",
    "            real_images = add_instance_noise(real_images)\n",
    "            fake_images = add_instance_noise(fake_images)\n",
    "\n",
    "        # Real images\n",
    "        labels_real = torch.ones(real_images.size(0), 1).to(device)\n",
    "        if do_smoothing:\n",
    "            labels_real = smooth_positive_labels(labels_real)\n",
    "\n",
    "        outputs_real = discriminator(real_images, labels)\n",
    "\n",
    "        loss_real = criterion(outputs_real, labels_real)\n",
    "\n",
    "        labels_fake = torch.zeros(batch_size, 1).to(device)\n",
    "        if do_smoothing:\n",
    "            labels_fake = smooth_negative_labels(labels_fake)\n",
    "\n",
    "        outputs_fake = discriminator(fake_images.detach(), labels)\n",
    "        loss_fake = criterion(outputs_fake, labels_fake)\n",
    "\n",
    "        # Combine losses and update Discriminator\n",
    "        loss_d = loss_real + loss_fake\n",
    "        loss_d.backward()\n",
    "        optimizer_d.step()\n",
    "\n",
    "    return loss_d\n",
    "\n",
    "\n",
    "def train_conditional_generator(\n",
    "    discriminator, fake_images, labels, criterion, optimizer_g, batch_size\n",
    "):\n",
    "    # Generator training\n",
    "    optimizer_g.zero_grad()\n",
    "\n",
    "    outputs = discriminator(fake_images, labels)\n",
    "    labels_g = torch.ones(batch_size, 1).to(device)\n",
    "\n",
    "    # Update Generator\n",
    "    loss_g = criterion(outputs, labels_g)\n",
    "    loss_g.backward()\n",
    "    optimizer_g.step()\n",
    "    return loss_g\n",
    "\n",
    "\n",
    "def train_conditional_model(\n",
    "    discriminator,\n",
    "    generator,\n",
    "    train_loader,\n",
    "    criterion,\n",
    "    optimizer_d,\n",
    "    optimizer_g,\n",
    "    epochs,\n",
    "    k,\n",
    "    batch_size,\n",
    "    noise_size,\n",
    "    num_classes,\n",
    "    do_smoothing_label=False,\n",
    "    do_instance_noise=False,\n",
    "    log_samples=False,\n",
    "    log_interval=10,\n",
    "):\n",
    "    losses_d = []\n",
    "    losses_g = []\n",
    "\n",
    "    # Training loop\n",
    "    with tqdm(total=epochs) as pbar:\n",
    "        for epoch in range(epochs):\n",
    "            for _, (real_images, real_labels) in enumerate(train_loader):\n",
    "                # Move tensors to the device\n",
    "                real_images = real_images.to(device)\n",
    "                real_labels = real_labels.to(device)\n",
    "\n",
    "                noise = torch.randn(batch_size, noise_size).to(device)\n",
    "                random_labels = torch.randint(0, num_classes, (batch_size,)).to(\n",
    "                    device\n",
    "                )  # Random labels for fake images\n",
    "\n",
    "                fake_images = generator(noise, random_labels).to(device)\n",
    "\n",
    "                loss_d = train_conditional_discriminator(\n",
    "                    discriminator,\n",
    "                    real_images,\n",
    "                    real_labels,\n",
    "                    fake_images,\n",
    "                    criterion,\n",
    "                    optimizer_d,\n",
    "                    k,\n",
    "                    batch_size,\n",
    "                    do_smoothing=do_smoothing_label,\n",
    "                    do_instance_noise=do_instance_noise,\n",
    "                )\n",
    "                loss_g = train_conditional_generator(\n",
    "                    discriminator,\n",
    "                    fake_images,\n",
    "                    random_labels,\n",
    "                    criterion,\n",
    "                    optimizer_g,\n",
    "                    batch_size,\n",
    "                )\n",
    "\n",
    "            pbar.update(1)\n",
    "            pbar.set_postfix(\n",
    "                {\"discriminator_loss\": loss_d.item(), \"generator_loss\": loss_g.item()},\n",
    "                refresh=True,\n",
    "            )\n",
    "\n",
    "            losses_d.append(loss_d.item())\n",
    "            losses_g.append(loss_g.item())\n",
    "\n",
    "            if log_samples and epoch % log_interval == 0:\n",
    "                generate_conditional_fake_images(\n",
    "                    generator, noise_size, num_classes\n",
    "                )\n",
    "\n",
    "    return losses_d, losses_g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = 10\n",
    "\n",
    "# Hyperparameters\n",
    "learning_rate_d = 0.0001\n",
    "learning_rate_g = 0.0001\n",
    "batch_size = 64\n",
    "epochs = 100\n",
    "noise_size = 1000  # Dimension of random noise vector for Generator\n",
    "k = 1  # Number of steps to apply to the Discriminator\n",
    "img_size = 32  # Size of the image\n",
    "\n",
    "# Create the Discriminator model\n",
    "discriminator = ConditionalDiscriminator(1, img_size, num_classes).to(device)\n",
    "# Create the Generator model\n",
    "generator = ConditionalGenerator(noise_size, 1, img_size, num_classes).to(device)\n",
    "\n",
    "# Loss and optimizers\n",
    "criterion = (\n",
    "    nn.BCELoss()\n",
    ")  # Binary cross entropy loss https://pytorch.org/docs/stable/generated/torch.nn.BCELoss.html\n",
    "optimizer_d = torch.optim.Adam(discriminator.parameters(), lr=learning_rate_d)\n",
    "optimizer_g = torch.optim.Adam(generator.parameters(), lr=learning_rate_g)\n",
    "\n",
    "# Get the data loader\n",
    "transform = transforms.Compose(\n",
    "    [\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Resize((img_size, img_size)),\n",
    "        transforms.Normalize((0.5,), (0.5,)),\n",
    "    ]\n",
    ")\n",
    "train_loader = get_mnist_data_loader(transform, batch_size)\n",
    "\n",
    "# Train the model\n",
    "losses_d, losses_g = train_conditional_model(\n",
    "    discriminator,\n",
    "    generator,\n",
    "    train_loader,\n",
    "    criterion,\n",
    "    optimizer_d,\n",
    "    optimizer_g,\n",
    "    epochs,\n",
    "    k,\n",
    "    batch_size,\n",
    "    noise_size,\n",
    "    num_classes,\n",
    "    do_smoothing_label=True,\n",
    "    do_instance_noise=True,\n",
    "    log_samples=True,\n",
    "    log_interval=1,\n",
    ")\n",
    "plot_losses(losses_d, losses_g, do_avg=True, avg_window=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_conditional_fake_images(generator, noise_size, num_classes, 32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# generate only 1\n",
    "generate_conditional_fake_images(generator, noise_size, number_of_fake_samples, condition)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "### Equilibrium\n",
    "... Nash\n",
    "mode collapse\n",
    "https://arxiv.org/pdf/1511.06434v2.pdf\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GANs",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
